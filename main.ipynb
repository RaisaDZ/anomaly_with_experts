{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main_functions import calculate_loss, share_algorithm\n",
    "from nab_functions import get_scores, read_nab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_list = [\n",
    "    \"knncad\",\n",
    "    \"numentaTM\",\n",
    "    \"twitterADVec\",\n",
    "    \"skyline\",\n",
    "    \"earthgeckoSkyline\",\n",
    "    \"numenta\",\n",
    "    \"bayesChangePt\",\n",
    "    \"null\",\n",
    "    \"expose\",\n",
    "    \"relativeEntropy\",\n",
    "    \"htmjava\",\n",
    "    \"randomCutForest\",\n",
    "    \"random\",\n",
    "    \"contextOSE\",\n",
    "    \"windowedGaussian\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read predictions from NAB and calculate predictions of Fixed-share and Variable-share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(f\"NAB/results/numenta\")\n",
    "folder_list = [s for s in dir_list if any(xs in s for xs in [\"real\", \"artificial\"])]\n",
    "alpha_range = [0, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9]\n",
    "share_range = [\"Fixed\", \"Variable\"]\n",
    "for m, folder_name in enumerate(folder_list):\n",
    "    file_list = os.listdir(f\"NAB/results/numenta/{folder_name}\")\n",
    "    file_list = [i.replace(\"numenta\", \"\") for i in file_list]\n",
    "    for n, file_name in enumerate(file_list):\n",
    "        dt = read_nab(algorithm_list, folder_name, file_name)\n",
    "        score_experts = np.array(dt.filter(regex=\"^score\", axis=1))\n",
    "        assert score_experts.shape[1] == len(algorithm_list)\n",
    "        target = dt[\"label\"].values\n",
    "        scores_share = get_scores(target, score_experts, share_range, alpha_range)\n",
    "        dt = pd.merge(dt, scores_share, left_index=True, right_index=True, validate=\"1:1\")\n",
    "        dt[\"file_name\"] = file_name\n",
    "        dt[\"folder_name\"] = folder_name\n",
    "        if (m == 0) & (n == 0):\n",
    "            results = dt.copy()\n",
    "        else:\n",
    "            results = pd.concat([results, dt], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_list = results.filter(regex=\"^score\", axis=1).columns.tolist()\n",
    "alg_list = [i.replace(\"score_\", \"\") for i in alg_list]\n",
    "losses_log = results[[\"timestamp\", \"value\", \"label\", \"file_name\", \"folder_name\"]].copy()\n",
    "losses_square = results[[\"timestamp\", \"value\", \"label\", \"file_name\", \"folder_name\"]].copy()\n",
    "for alg_ind in alg_list:\n",
    "    losses_log[f\"loss_{alg_ind}\"] = calculate_loss(\n",
    "        results[\"label\"], results[f\"score_{alg_ind}\"], loss_type=\"log\"\n",
    "    )\n",
    "    losses_square[f\"loss_{alg_ind}\"] = calculate_loss(\n",
    "        results[\"label\"], results[f\"score_{alg_ind}\"], loss_type=\"square\"\n",
    "    )\n",
    "losses_log_total = losses_log.groupby([\"folder_name\", \"file_name\"])[\n",
    "    losses_log.filter(regex=\"^loss\", axis=1).columns.tolist()\n",
    "].sum()\n",
    "losses_square_total = losses_square.groupby([\"folder_name\", \"file_name\"])[\n",
    "    losses_square.filter(regex=\"^loss\", axis=1).columns.tolist()\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"results\"):\n",
    "    os.makedirs(\"results\")\n",
    "results.to_csv(f\"results/scores.csv\", index=False)\n",
    "losses_log.to_csv(f\"results/losses_log.csv\", index=False)\n",
    "losses_square.to_csv(f\"results/losses_square.csv\", index=False)\n",
    "losses_log_total.to_csv(f\"results/losses_log_total.csv\")\n",
    "losses_square_total.to_csv(f\"results/losses_square_total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the predictions of Fixed-share and Variable-share in the NAB format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_list = results.filter(regex=\"Fixed|Variable\", axis=1).columns.tolist()\n",
    "share_list = [i.replace(\"score_\", \"\") for i in share_list]\n",
    "for share_name in share_list:\n",
    "    for m, folder_name in enumerate(folder_list):\n",
    "        file_list = os.listdir(f\"NAB/results/numenta/{folder_name}\")\n",
    "        file_list = [i.replace(\"numenta\", \"\") for i in file_list]\n",
    "        for n, file_name in enumerate(file_list):\n",
    "            results_temp = results[\n",
    "                (results[\"folder_name\"] == folder_name) & (results[\"file_name\"] == file_name)\n",
    "            ]\n",
    "            if not os.path.exists(f\"NAB/results/{share_name}/{folder_name}\"):\n",
    "                os.makedirs(f\"NAB/results/{share_name}/{folder_name}\")\n",
    "            results_temp[[\"timestamp\", \"value\", f\"score_{share_name}\", \"label\"]].rename(\n",
    "                {f\"score_{share_name}\": \"anomaly_score\"}, axis=1\n",
    "            ).to_csv(\n",
    "                f\"NAB/results/{share_name}/{folder_name}/{share_name}{file_name}\",\n",
    "                index=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate NAB scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'run.py', '-d', 'Fixed0', 'Fixed5', 'Fixed10', 'Fixed20', 'Fixed30', 'Fixed50', 'Fixed70', 'Fixed90', 'Variable0', 'Variable5', 'Variable10', 'Variable20', 'Variable30', 'Variable50', 'Variable70', 'Variable90', 'knncad', 'numentaTM', 'twitterADVec', 'skyline', 'earthgeckoSkyline', 'numenta', 'bayesChangePt', 'null', 'expose', 'relativeEntropy', 'htmjava', 'randomCutForest', 'random', 'contextOSE', 'windowedGaussian', '--optimize', '--score', '--normalize'], returncode=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Path(\"NAB\") / \"results/final_results.json\").unlink()\n",
    "ps = subprocess.Popen([\"echo\", \"y\"], stdout=subprocess.PIPE)\n",
    "subprocess.run(\n",
    "    shlex.split(\n",
    "        f\"python run.py -d {' '.join(share_list+algorithm_list)} --optimize --score --normalize\"\n",
    "    ),\n",
    "    cwd=\"NAB\",\n",
    "    stdin=ps.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
